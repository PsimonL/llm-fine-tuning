{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:42:11.700537800Z",
     "start_time": "2025-05-03T16:42:11.618205200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kyre/repos/llm-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path(\"/\".join(__vsc_ipynb_file__.split(\"/\")[:-1]))\n",
    "print(NOTEBOOK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:42:27.943605900Z",
     "start_time": "2025-05-03T16:42:11.644206700Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data subsets: dict_keys(['train', 'test'])\n",
      "Features: \n",
      "---> id                            : 5097\n",
      "---> domain                        : forestry\n",
      "---> domain_description            : Comprehensive data on sustainable forest management, timber production, wildlife habitat, and carbon sequestration in forestry.\n",
      "---> sql_complexity                : single join\n",
      "---> sql_complexity_description    : only one join (specify inner, outer, cross)\n",
      "---> sql_task_type                 : analytics and reporting\n",
      "---> sql_task_type_description     : generating reports, dashboards, and analytical insights\n",
      "---> sql_prompt                    : What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "---> sql_context                   : CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "---> sql                           : SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "---> sql_explanation               : Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/gretelai/synthetic_text_to_sql\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "print(\"Available data subsets:\", dataset.keys())\n",
    "print(\"Features: \")\n",
    "for k, v in dataset[\"train\"][0].items():\n",
    "    print(f\"---> {k:30}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:43:15.028542700Z",
     "start_time": "2025-05-03T16:42:27.948187200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation']\n",
      "Checking 'train' split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:07<00:07,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql_prompt: 0 missing or empty entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql: 0 missing or empty entries\n",
      "\n",
      "Checking 'test' split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql_prompt: 0 missing or empty entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sql: 0 missing or empty entries\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def check_missing_data(dataset, columns):\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Checking '{split}' split...\")\n",
    "        for column in tqdm(columns):\n",
    "            if column not in dataset[split].column_names:\n",
    "                print(f\"  Column '{column}' not found in the dataset!\")\n",
    "                continue\n",
    "            missing_count = sum(1 for example in dataset[split] if not example[column] or example[column].strip() == \"\")\n",
    "            print(f\"  {column}: {missing_count} missing or empty entries\")\n",
    "        print()\n",
    "\n",
    "print(\"Dataset columns:\", dataset[\"train\"].column_names)\n",
    "check_missing_data(dataset, columns=[\"sql_prompt\", \"sql\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ShareGPT formatted data to 'data/dataset_train.json'\n",
      "\n",
      "--- First Record Example ---\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"Context:\\n'CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');'\\n                                    Question:\\n'What is the total volume of timber sold by each salesperson, sorted by salesperson?'\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"Result: 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;'\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": \"You are a helpful assistant specialized in generating SQL queries.\\nGenerate an SQL query that correctly answers the user's question based on the provided database schema and context.\",\n",
      "  \"tools\": \"\"\n",
      "}\n",
      "mkdir: cannot create directory ‘/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/data/’: File exists\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import create_sharegpt_format\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "sharegpt_data = create_sharegpt_format(train_df)\n",
    "\n",
    "output_filename = Path('data/dataset_train.json')\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sharegpt_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Successfully saved ShareGPT formatted data to '{output_filename}'\")\n",
    "\n",
    "    if sharegpt_data:\n",
    "        print(\"\\n--- First Record Example ---\")\n",
    "        print(json.dumps(sharegpt_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")\n",
    "\n",
    "!mkdir $NOTEBOOK_DIR/LLaMA-Factory/data/\n",
    "!cp -r $NOTEBOOK_DIR/data/ $NOTEBOOK_DIR/LLaMA-Factory/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2202\n"
     ]
    }
   ],
   "source": [
    "max_len_str = 0\n",
    "for record in sharegpt_data:\n",
    "    max_len_str = max(max_len_str, len(record['conversations'][0]['value']))\n",
    "print(max_len_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n",
      "/home/kyre/repos/llm-fine-tuning/LLaMA-Factory\n",
      "CITATION.cff    Makefile      \u001b[0m\u001b[01;34mcache\u001b[0m/       \u001b[01;34mexamples\u001b[0m/         setup.py\n",
      "LICENSE         README.md     \u001b[01;34mdata\u001b[0m/        pyproject.toml    \u001b[01;34msrc\u001b[0m/\n",
      "\u001b[01;34mLLaMA-Factory\u001b[0m/  README_zh.md  \u001b[01;34mdocker\u001b[0m/      requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
      "MANIFEST.in     \u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  \u001b[01;34mscripts\u001b[0m/          train_qwen3.json\n",
      "Obtaining file:///home/kyre/repos/llm-fine-tuning/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (4.51.3)\n",
      "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (5.25.0)\n",
      "Requirement already satisfied: scipy in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (1.15.2)\n",
      "Requirement already satisfied: einops in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (6.30.2)\n",
      "Requirement already satisfied: uvicorn in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.34.2)\n",
      "Requirement already satisfied: fastapi in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
      "Requirement already satisfied: sse-starlette in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (3.10.1)\n",
      "Requirement already satisfied: fire in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: omegaconf in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (2.2.3)\n",
      "Requirement already satisfied: av in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (14.3.0)\n",
      "Requirement already satisfied: librosa in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
      "Requirement already satisfied: tyro<0.9.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (2.7.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from llamafactory==0.9.3.dev0) (0.45.5)\n",
      "Requirement already satisfied: psutil in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (7.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.18)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
      "Requirement already satisfied: pydub in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.27.2)\n",
      "Requirement already satisfied: setuptools in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (80.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.16.0)\n",
      "Requirement already satisfied: termcolor in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (0.61.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
      "Requirement already satisfied: certifi in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26913 sha256=ee7e9e84513478e8386e0cb9cfbe080e55e1c6b5218b58cf68f35548d6813404\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4fhcjjeg/wheels/70/03/0b/655beed7ca3638e1398a712a992d0b92040a36b4eb41cc5ee3\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.3.dev0\n",
      "    Uninstalling llamafactory-0.9.3.dev0:\n",
      "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
      "Successfully installed llamafactory-0.9.3.dev0\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd $NOTEBOOK_DIR/LLaMA-Factory\n",
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://11bfd50757f61dfa03.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "gio: https://11bfd50757f61dfa03.gradio.live: Operation not supported\n",
      "Keyboard interruption in main thread... closing server.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/gradio/blocks.py\", line 2997, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/gradio/blocks.py\", line 2903, in launch\n",
      "    self.block_thread()\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/gradio/blocks.py\", line 3001, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1151, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1167, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Killing tunnel 0.0.0.0:7860 <> https://11bfd50757f61dfa03.gradio.live\n"
     ]
    }
   ],
   "source": [
    "# !GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    # \"deepspeed\": DS_CONFIG_PATH,\n",
    "    \"cutoff_len\": 1024,\n",
    "    \"dataset\": \"train_sql_dataset\",\n",
    "    \"ddp_timeout\": 9000,\n",
    "    \"do_train\": True,\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"use_dora\": True,\n",
    "    \"fp16\": True,\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"flash_attn\": \"fa2\",\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"logging_steps\": 8,\n",
    "    \"lora_target\": \"q_proj,v_proj\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"output_dir\": \"out\",\n",
    "    \"overwrite_cache\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"plot_loss\": True,\n",
    "    \"report_to\": None, # wnadb\n",
    "    \"save_steps\": 1000,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"stage\": \"sft\",\n",
    "    \"template\": \"qwen3\",\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "\n",
    "json.dump(training_args, open(f\"{NOTEBOOK_DIR}/LLaMA-Factory/train_qwen3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-05-04 03:01:44] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file vocab.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file merges.txt from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file tokenizer.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file tokenizer_config.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:44,457 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-04 03:01:44,705 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:693] 2025-05-04 03:01:45,263 >> loading configuration file config.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-04 03:01:45,266 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file vocab.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file merges.txt from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file tokenizer.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file tokenizer_config.json from cache at /home/kyre/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/6130ef31402718485ca4d80a6234f70d9a4cf362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-04 03:01:45,404 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-04 03:01:45,656 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/.env/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/data/loader.py\", line 304, in get_dataset\n",
      "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/data/loader.py\", line 178, in _get_merged_dataset\n",
      "    for dataset_name, dataset_attr in zip(dataset_names, get_dataset_list(dataset_names, data_args.dataset_dir)):\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kyre/repos/llm-fine-tuning/LLaMA-Factory/src/llamafactory/data/parser.py\", line 129, in get_dataset_list\n",
      "    raise ValueError(f\"Undefined dataset {name} in {DATA_CONFIG}.\")\n",
      "ValueError: Undefined dataset dataset_train.json in dataset_info.json.\n"
     ]
    }
   ],
   "source": [
    "!cd $NOTEBOOK_DIR/LLaMA-Factory\n",
    "# !llamafactory-cli train train_qwen3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:43:15.394083700Z",
     "start_time": "2025-05-03T16:43:15.004673300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def normalize_text(text):\n",
    "#     text = text.lower().strip().replace(\"\\n\", \" \")\n",
    "#     return text\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:45:39.565195200Z",
     "start_time": "2025-05-03T16:43:15.399414900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5851/5851 [00:02<00:00, 2664.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# def preprocess(example):\n",
    "#     input_text = \"Translate to SQL: \" + normalize_text(example[\"sql_prompt\"])\n",
    "#     output_text = normalize_text(example[\"sql\"])\n",
    "\n",
    "#     model_inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "#     labels = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "# tokenized_ds = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# available_splits = dataset.keys()\n",
    "# train_ds = tokenized_ds[\"train\"]\n",
    "# validation_ds = tokenized_ds[\"test\"] if \"test\" in available_splits else None\n",
    "# test_ds = tokenized_ds[\"test\"] if \"test\" in available_splits else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:47:35.804666900Z",
     "start_time": "2025-05-03T16:45:39.573204300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_956164/3438895915.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1082' max='62500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1082/62500 01:25 < 1:21:16, 12.59 it/s, Epoch 0.09/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m      3\u001b[39m training_args = Seq2SeqTrainingArguments(\n\u001b[32m      4\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./sql_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Changed from evaluation_strategy\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     logging_dir=\u001b[33m\"\u001b[39m\u001b[33m./logs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     17\u001b[39m     model=model,\n\u001b[32m     18\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     tokenizer=tokenizer\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1905\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1902\u001b[39m         decoder_attention_mask = decoder_attention_mask.to(\u001b[38;5;28mself\u001b[39m.decoder.first_device)\n\u001b[32m   1904\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1905\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1921\u001b[39m sequence_output = decoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1114\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1115\u001b[39m         layer_module.forward,\n\u001b[32m   1116\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1128\u001b[39m         cache_position,\n\u001b[32m   1129\u001b[39m     )\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1148\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:706\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    704\u001b[39m do_cross_attention = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     cross_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     hidden_states, past_key_value = cross_attention_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:636\u001b[39m, in \u001b[36mT5LayerCrossAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    623\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    624\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    634\u001b[39m ):\n\u001b[32m    635\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     layer_output = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    649\u001b[39m     outputs = (layer_output,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[32m    559\u001b[39m attn_weights = nn.functional.softmax(scores.float(), dim=-\u001b[32m1\u001b[39m).type_as(scores)\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m attn_weights = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/llm-fine-tuning/.env/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./sql_model\",\n",
    "#     eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     predict_with_generate=True,\n",
    "#     logging_dir=\"./logs\"\n",
    "# )\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=validation_ds,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T16:47:35.789336800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def evaluate_model(test_dataset):\n",
    "#     metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "#     def compute_metrics(eval_pred):\n",
    "#         predictions, labels = eval_pred\n",
    "#         decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#         decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#         decoded_labels = [[label] for label in decoded_labels]\n",
    "#         result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#         return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "#     results = trainer.evaluate(eval_dataset=test_dataset, compute_metrics=compute_metrics)\n",
    "#     return results\n",
    "\n",
    "# if test_ds:\n",
    "#     test_results = evaluate_model(test_ds)\n",
    "#     print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-03T16:47:35.790340500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def driver(question):\n",
    "#     inputs = tokenizer(\"Translate to SQL: \" + question, return_tensors=\"pt\").input_ids\n",
    "#     outputs = model.generate(inputs, max_length=128)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(driver(\"Find all customers who ordered in 2023\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
